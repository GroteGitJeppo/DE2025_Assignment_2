{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark import SparkConf\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql import Window\n",
        "\n",
        "sparkConf = SparkConf()\n",
        "sparkConf.setMaster(\"spark://spark-master:7077\")\n",
        "sparkConf.setAppName(\"FastFood_Batch_Analytics\")\n",
        "sparkConf.set(\"spark.driver.memory\", \"2g\")\n",
        "sparkConf.set(\"spark.executor.cores\", \"1\")\n",
        "sparkConf.set(\"spark.driver.cores\", \"1\")\n",
        "\n",
        "# create the Spark session, which is the entry point to the Spark SQL engine.\n",
        "spark = SparkSession.builder.config(conf=sparkConf).getOrCreate()\n",
        "\n",
        "conf = spark.sparkContext._jsc.hadoopConfiguration()\n",
        "conf.set(\"fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\n",
        "conf.set(\"fs.AbstractFileSystem.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\")\n",
        "\n",
        "# Read the fast food ordering dataset\n",
        "gsc_file_path = 'gs://data_de2025_2062061/fast_food_ordering_dataset.csv'\n",
        "\n",
        "df = spark.read.format(\"csv\").option(\"header\", \"true\") \\\n",
        "    .load(gsc_file_path)\n",
        "\n",
        "# Data type conversions\n",
        "df = df.withColumn(\"order_value\", col(\"order_value\").cast(\"double\")) \\\n",
        "       .withColumn(\"delivery_time_minutes\", col(\"delivery_time_minutes\").cast(\"integer\")) \\\n",
        "       .withColumn(\"items_count\", col(\"items_count\").cast(\"integer\")) \\\n",
        "       .withColumn(\"order_time\", to_timestamp(col(\"order_time\"), \"yyyy-MM-dd HH:mm:ss\"))\n",
        "\n",
        "df.printSchema()\n",
        "df.show(10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Transformation 1: Revenue Analysis by City\n",
        "\n",
        "Calculate total revenue, average order value, total orders, and average delivery time for each city.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Revenue Analysis by City\n",
        "revenue_by_city = df.groupBy(\"city\") \\\n",
        "    .agg(\n",
        "        sum(\"order_value\").alias(\"total_revenue\"),\n",
        "        avg(\"order_value\").alias(\"avg_order_value\"),\n",
        "        count(\"*\").alias(\"total_orders\"),\n",
        "        avg(\"delivery_time_minutes\").alias(\"avg_delivery_time\")\n",
        "    ) \\\n",
        "    .orderBy(desc(\"total_revenue\"))\n",
        "\n",
        "revenue_by_city.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Transformation 2: Cuisine Type Performance\n",
        "\n",
        "Analyze performance metrics for each cuisine type including order count, revenue, and delivery times.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cuisine Type Performance\n",
        "cuisine_performance = df.groupBy(\"cuisine_type\") \\\n",
        "    .agg(\n",
        "        count(\"*\").alias(\"order_count\"),\n",
        "        sum(\"order_value\").alias(\"total_revenue\"),\n",
        "        avg(\"order_value\").alias(\"avg_order_value\"),\n",
        "        avg(\"items_count\").alias(\"avg_items_per_order\"),\n",
        "        avg(\"delivery_time_minutes\").alias(\"avg_delivery_time\")\n",
        "    ) \\\n",
        "    .orderBy(desc(\"total_revenue\"))\n",
        "\n",
        "cuisine_performance.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Transformation 3: Payment Method Analysis\n",
        "\n",
        "Analyze payment method usage patterns and revenue contribution.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Payment Method Analysis\n",
        "payment_analysis = df.groupBy(\"payment_method\") \\\n",
        "    .agg(\n",
        "        count(\"*\").alias(\"usage_count\"),\n",
        "        sum(\"order_value\").alias(\"total_revenue\"),\n",
        "        avg(\"order_value\").alias(\"avg_order_value\")\n",
        "    ) \\\n",
        "    .orderBy(desc(\"usage_count\"))\n",
        "\n",
        "payment_analysis.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Transformation 4: Top Performing Cities by Cuisine (Window Functions)\n",
        "\n",
        "Use window functions to rank cities by revenue within each cuisine type and identify top 3 performers.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Top Performing Cities by Cuisine (Window Functions)\n",
        "window_spec = Window.partitionBy(\"cuisine_type\").orderBy(desc(\"total_revenue\"))\n",
        "\n",
        "city_cuisine_revenue = df.groupBy(\"city\", \"cuisine_type\") \\\n",
        "    .agg(sum(\"order_value\").alias(\"total_revenue\")) \\\n",
        "    .withColumn(\"rank\", dense_rank().over(window_spec)) \\\n",
        "    .filter(col(\"rank\") <= 3) \\\n",
        "    .orderBy(\"cuisine_type\", \"rank\")\n",
        "\n",
        "city_cuisine_revenue.show(50)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Transformation 5: Daily Revenue Trends\n",
        "\n",
        "Analyze revenue trends over time by aggregating orders by date.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Daily Revenue Trends\n",
        "daily_revenue = df.withColumn(\"order_date\", to_date(col(\"order_time\"))) \\\n",
        "    .groupBy(\"order_date\") \\\n",
        "    .agg(\n",
        "        sum(\"order_value\").alias(\"daily_revenue\"),\n",
        "        count(\"*\").alias(\"daily_orders\"),\n",
        "        avg(\"order_value\").alias(\"avg_order_value\")\n",
        "    ) \\\n",
        "    .orderBy(\"order_date\")\n",
        "\n",
        "daily_revenue.show(30)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Transformation 6: High-Value Orders Analysis\n",
        "\n",
        "Identify top 10% of orders by value using percentile ranking.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# High-Value Orders Analysis (Top 10%)\n",
        "window_high_value = Window.orderBy(desc(\"order_value\"))\n",
        "high_value_orders = df.withColumn(\"percent_rank\", percent_rank().over(window_high_value)) \\\n",
        "    .filter(col(\"percent_rank\") <= 0.1) \\\n",
        "    .select(\"order_id\", \"city\", \"cuisine_type\", \"order_value\", \"items_count\", \"payment_method\")\n",
        "\n",
        "high_value_orders.show(20)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Transformation 7: Delivery Performance by City\n",
        "\n",
        "Analyze delivery efficiency metrics and categorize cities by delivery speed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Delivery Performance by City\n",
        "delivery_performance = df.groupBy(\"city\") \\\n",
        "    .agg(\n",
        "        avg(\"delivery_time_minutes\").alias(\"avg_delivery_time\"),\n",
        "        min(\"delivery_time_minutes\").alias(\"min_delivery_time\"),\n",
        "        max(\"delivery_time_minutes\").alias(\"max_delivery_time\"),\n",
        "        count(\"*\").alias(\"total_orders\")\n",
        "    ) \\\n",
        "    .withColumn(\"delivery_efficiency\", \n",
        "                when(col(\"avg_delivery_time\") < 35, \"Fast\")\n",
        "                .when(col(\"avg_delivery_time\") < 50, \"Medium\")\n",
        "                .otherwise(\"Slow\")) \\\n",
        "    .orderBy(\"avg_delivery_time\")\n",
        "\n",
        "delivery_performance.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup Hadoop FS configuration for schema gs://\n",
        "conf = spark.sparkContext._jsc.hadoopConfiguration()\n",
        "conf.set(\"fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\n",
        "conf.set(\"fs.AbstractFileSystem.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\")\n",
        "\n",
        "# Use the Cloud Storage bucket for temporary BigQuery export data that the connector uses.\n",
        "bucket = \"temp_de2025_2062061\"  \n",
        "spark.conf.set('temporaryGcsBucket', bucket)\n",
        "\n",
        "# Save revenue_by_city to BigQuery\n",
        "revenue_by_city.write.format('bigquery') \\\n",
        "  .option('table', 'de2025-472319.labdataset.revenue_by_city') \\\n",
        "  .mode(\"overwrite\") \\\n",
        "  .save()\n",
        "\n",
        "# Save cuisine_performance to BigQuery\n",
        "cuisine_performance.write.format('bigquery') \\\n",
        "  .option('table', 'de2025-472319.labdataset.cuisine_performance') \\\n",
        "  .mode(\"overwrite\") \\\n",
        "  .save()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Stop the Spark context\n",
        "spark.stop()\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
