{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Pipeline 2: Stream Order Monitoring\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark import SparkConf\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType, TimestampType\n",
        "from time import sleep\n",
        "\n",
        "sparkConf = SparkConf()\n",
        "sparkConf.setMaster(\"spark://spark-master:7077\")\n",
        "sparkConf.setAppName(\"FastFood_Stream_Monitoring\")\n",
        "sparkConf.set(\"spark.driver.memory\", \"2g\")\n",
        "sparkConf.set(\"spark.executor.cores\", \"1\")\n",
        "sparkConf.set(\"spark.driver.cores\", \"1\")\n",
        "\n",
        "spark = SparkSession.builder.config(conf=sparkConf).getOrCreate()\n",
        "\n",
        "dataSchema = StructType([\n",
        "    StructField(\"order_id\", StringType(), True),\n",
        "    StructField(\"order_time\", StringType(), True),\n",
        "    StructField(\"city\", StringType(), True),\n",
        "    StructField(\"cuisine_type\", StringType(), True),\n",
        "    StructField(\"order_value\", DoubleType(), True),\n",
        "    StructField(\"delivery_time_minutes\", IntegerType(), True),\n",
        "    StructField(\"payment_method\", StringType(), True),\n",
        "    StructField(\"items_count\", IntegerType(), True)\n",
        "])\n",
        "\n",
        "sdf_raw = spark.readStream \\\n",
        "    .schema(dataSchema) \\\n",
        "    .option(\"maxFilesPerTrigger\", 1) \\\n",
        "    .option(\"latestFirst\", \"false\") \\\n",
        "    .csv(\"/home/jovyan/data/fast_food_ordering_dataset_stream\")\n",
        "\n",
        "sdf = sdf_raw.withColumn(\"order_time\", to_timestamp(col(\"order_time\"), \"yyyy-MM-dd HH:mm:ss\"))\n",
        "\n",
        "sdf.printSchema()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Streaming Transformation 1: Real-time Orders by City\n",
        "\n",
        "Monitor the count of orders per city in real-time.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for query in spark.streams.active:\n",
        "    if query.name == \"city_order_counts\":\n",
        "        query.stop()\n",
        "\n",
        "city_counts = sdf.groupBy(\"city\").count()\n",
        "\n",
        "city_query = city_counts.writeStream \\\n",
        "    .queryName(\"city_order_counts\") \\\n",
        "    .format(\"memory\") \\\n",
        "    .outputMode(\"complete\") \\\n",
        "    .trigger(processingTime='2 seconds') \\\n",
        "    .start()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Streaming Transformation 2: Average Order Value by Cuisine Type\n",
        "\n",
        "Calculate average order value by cuisine type in 30-day time windows.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with_event_time = sdf.selectExpr(\"*\", \"order_time as event_time\")\n",
        "\n",
        "for query in spark.streams.active:\n",
        "    if query.name == \"cuisine_order_value\":\n",
        "        query.stop()\n",
        "\n",
        "cuisine_order_value = with_event_time \\\n",
        "    .withWatermark(\"event_time\", \"60 days\") \\\n",
        "    .groupBy(window(col(\"event_time\"), \"30 days\"), \"cuisine_type\") \\\n",
        "    .agg(avg(\"order_value\").alias(\"avg_order_value\"), sum(\"order_value\").alias(\"total_revenue\"), count(\"*\").alias(\"order_count\"))\n",
        "\n",
        "cuisine_query = cuisine_order_value.writeStream \\\n",
        "    .queryName(\"cuisine_order_value\") \\\n",
        "    .format(\"memory\") \\\n",
        "    .outputMode(\"complete\") \\\n",
        "    .trigger(processingTime='2 seconds') \\\n",
        "    .start()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Streaming Transformation 3: Payment Method Distribution\n",
        "\n",
        "Calculate payment method distribution in 30-day time windows.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for query in spark.streams.active:\n",
        "    if query.name == \"payment_method_distribution\":\n",
        "        query.stop()\n",
        "\n",
        "payment_distribution = with_event_time \\\n",
        "    .withWatermark(\"event_time\", \"60 days\") \\\n",
        "    .groupBy(window(col(\"event_time\"), \"30 days\"), \"payment_method\") \\\n",
        "    .agg(count(\"*\").alias(\"order_count\"), sum(\"order_value\").alias(\"total_value\"))\n",
        "\n",
        "payment_query = payment_distribution.writeStream \\\n",
        "    .queryName(\"payment_method_distribution\") \\\n",
        "    .format(\"memory\") \\\n",
        "    .outputMode(\"complete\") \\\n",
        "    .trigger(processingTime='2 seconds') \\\n",
        "    .start()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Streaming Transformation 4: Total Revenue by City\n",
        "\n",
        "Calculate total revenue by city in 30-day time windows.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for query in spark.streams.active:\n",
        "    if query.name == \"city_revenue\":\n",
        "        query.stop()\n",
        "\n",
        "city_revenue = with_event_time \\\n",
        "    .withWatermark(\"event_time\", \"60 days\") \\\n",
        "    .groupBy(window(col(\"event_time\"), \"30 days\"), \"city\") \\\n",
        "    .agg(sum(\"order_value\").alias(\"total_revenue\"), avg(\"order_value\").alias(\"avg_order_value\"), count(\"*\").alias(\"order_count\"))\n",
        "\n",
        "revenue_query = city_revenue.writeStream \\\n",
        "    .queryName(\"city_revenue\") \\\n",
        "    .format(\"memory\") \\\n",
        "    .outputMode(\"complete\") \\\n",
        "    .trigger(processingTime='2 seconds') \\\n",
        "    .start()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Monitor Streaming Queries\n",
        "\n",
        "Display results from all streaming queries. The queries will update as new data arrives.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    for x in range(20):\n",
        "        print(f\"\\n=== Iteration {x+1} ===\")\n",
        "        \n",
        "        print(\"\\n--- City Order Counts (Real-time) ---\")\n",
        "        spark.sql(\"SELECT * FROM city_order_counts ORDER BY count DESC\").show()\n",
        "        \n",
        "        print(\"\\n--- Average Order Value by Cuisine Type (30-day windows) ---\")\n",
        "        spark.sql(\"\"\"\n",
        "            SELECT \n",
        "                window,\n",
        "                cuisine_type,\n",
        "                avg_order_value,\n",
        "                total_revenue,\n",
        "                order_count\n",
        "            FROM cuisine_order_value\n",
        "            ORDER BY window DESC, total_revenue DESC\n",
        "            LIMIT 15\n",
        "        \"\"\").show(truncate=False)\n",
        "        \n",
        "        print(\"\\n--- Payment Method Distribution (30-day windows) ---\")\n",
        "        spark.sql(\"\"\"\n",
        "            SELECT \n",
        "                window,\n",
        "                payment_method,\n",
        "                order_count,\n",
        "                total_value,\n",
        "                ROUND(total_value / SUM(total_value) OVER (PARTITION BY window), 4) * 100 as percentage_of_total\n",
        "            FROM payment_method_distribution\n",
        "            ORDER BY window DESC, order_count DESC\n",
        "            LIMIT 15\n",
        "        \"\"\").show(truncate=False)\n",
        "        \n",
        "        print(\"\\n--- Total Revenue by City (30-day windows) ---\")\n",
        "        spark.sql(\"\"\"\n",
        "            SELECT \n",
        "                window,\n",
        "                city,\n",
        "                total_revenue,\n",
        "                avg_order_value,\n",
        "                order_count\n",
        "            FROM city_revenue\n",
        "            ORDER BY window DESC, total_revenue DESC\n",
        "            LIMIT 15\n",
        "        \"\"\").show(truncate=False)\n",
        "        \n",
        "        sleep(10)\n",
        "        \n",
        "except KeyboardInterrupt:\n",
        "    city_query.stop()\n",
        "    cuisine_query.stop()\n",
        "    payment_query.stop()\n",
        "    revenue_query.stop()\n",
        "    spark.stop()\n",
        "except Exception as e:\n",
        "    city_query.stop()\n",
        "    cuisine_query.stop()\n",
        "    payment_query.stop()\n",
        "    revenue_query.stop()\n",
        "    spark.stop()\n",
        "    print(f\"Error: {e}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Stop the Spark context\n",
        "spark.stop()"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
