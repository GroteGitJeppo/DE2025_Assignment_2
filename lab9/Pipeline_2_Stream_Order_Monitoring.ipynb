{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Pipeline 2: Stream Order Monitoring\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark import SparkConf\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType, TimestampType\n",
        "from time import sleep\n",
        "\n",
        "sparkConf = SparkConf()\n",
        "sparkConf.setMaster(\"spark://spark-master:7077\")\n",
        "sparkConf.setAppName(\"FastFood_Stream_Monitoring\")\n",
        "sparkConf.set(\"spark.driver.memory\", \"2g\")\n",
        "sparkConf.set(\"spark.executor.cores\", \"1\")\n",
        "sparkConf.set(\"spark.driver.cores\", \"1\")\n",
        "\n",
        "spark = SparkSession.builder.config(conf=sparkConf).getOrCreate()\n",
        "\n",
        "dataSchema = StructType([\n",
        "    StructField(\"order_id\", StringType(), True),\n",
        "    StructField(\"order_time\", StringType(), True),\n",
        "    StructField(\"city\", StringType(), True),\n",
        "    StructField(\"cuisine_type\", StringType(), True),\n",
        "    StructField(\"order_value\", DoubleType(), True),\n",
        "    StructField(\"delivery_time_minutes\", IntegerType(), True),\n",
        "    StructField(\"payment_method\", StringType(), True),\n",
        "    StructField(\"items_count\", IntegerType(), True)\n",
        "])\n",
        "\n",
        "sdf_raw = spark.readStream \\\n",
        "    .schema(dataSchema) \\\n",
        "    .option(\"maxFilesPerTrigger\", 1) \\\n",
        "    .option(\"latestFirst\", \"false\") \\\n",
        "    .csv(\"/home/jovyan/data/fast_food_ordering_dataset_stream\")\n",
        "\n",
        "sdf = sdf_raw.withColumn(\"order_time\", to_timestamp(col(\"order_time\"), \"yyyy-MM-dd HH:mm:ss\"))\n",
        "\n",
        "sdf.printSchema()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Streaming Transformation 1: Real-time Orders by City\n",
        "\n",
        "Monitor the count of orders per city in real-time.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for query in spark.streams.active:\n",
        "    if query.name == \"city_order_counts\":\n",
        "        query.stop()\n",
        "\n",
        "city_counts = sdf.groupBy(\"city\").count()\n",
        "\n",
        "city_query = city_counts.writeStream \\\n",
        "    .queryName(\"city_order_counts\") \\\n",
        "    .format(\"memory\") \\\n",
        "    .outputMode(\"complete\") \\\n",
        "    .trigger(processingTime='2 seconds') \\\n",
        "    .start()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Streaming Transformation 2: Windowed Average Delivery Time by City\n",
        "\n",
        "Calculate average delivery time by city in 30-day time windows.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with_event_time = sdf.selectExpr(\"*\", \"order_time as event_time\")\n",
        "\n",
        "for query in spark.streams.active:\n",
        "    if query.name == \"delivery_time_window\":\n",
        "        query.stop()\n",
        "\n",
        "delivery_window = with_event_time \\\n",
        "    .withWatermark(\"event_time\", \"60 days\") \\\n",
        "    .groupBy(window(col(\"event_time\"), \"30 days\"), \"city\") \\\n",
        "    .agg(avg(\"delivery_time_minutes\").alias(\"avg_delivery_time\"), count(\"*\").alias(\"orders_in_window\"))\n",
        "\n",
        "delivery_query = delivery_window.writeStream \\\n",
        "    .queryName(\"delivery_time_window\") \\\n",
        "    .format(\"memory\") \\\n",
        "    .outputMode(\"complete\") \\\n",
        "    .trigger(processingTime='2 seconds') \\\n",
        "    .start()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Streaming Transformation 3: Monthly Average Delivery Time by City\n",
        "\n",
        "Calculate average delivery time by city in 30-day time windows.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for query in spark.streams.active:\n",
        "    if query.name == \"monthly_delivery_time\":\n",
        "        query.stop()\n",
        "\n",
        "monthly_delivery_time = with_event_time \\\n",
        "    .withWatermark(\"event_time\", \"60 days\") \\\n",
        "    .groupBy(window(col(\"event_time\"), \"30 days\"), \"city\") \\\n",
        "    .agg(avg(\"delivery_time_minutes\").alias(\"avg_delivery_time\"), count(\"*\").alias(\"monthly_orders\"))\n",
        "\n",
        "monthly_delivery_time_query = monthly_delivery_time.writeStream \\\n",
        "    .queryName(\"monthly_delivery_time\") \\\n",
        "    .format(\"memory\") \\\n",
        "    .outputMode(\"complete\") \\\n",
        "    .trigger(processingTime='2 seconds') \\\n",
        "    .start()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Monitor Streaming Queries\n",
        "\n",
        "Display results from all streaming queries. The queries will update as new data arrives.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    for x in range(20):\n",
        "        print(f\"\\n=== Iteration {x+1} ===\")\n",
        "        \n",
        "        print(\"\\n--- City Order Counts ---\")\n",
        "        spark.sql(\"SELECT * FROM city_order_counts ORDER BY count DESC\").show()\n",
        "        \n",
        "        print(\"\\n--- Delivery Time by City (30-day windows) ---\")\n",
        "        spark.sql(\"\"\"\n",
        "            WITH ordered_windows AS (\n",
        "                SELECT \n",
        "                    window,\n",
        "                    city,\n",
        "                    avg_delivery_time,\n",
        "                    orders_in_window,\n",
        "                    window.start as window_start\n",
        "                FROM delivery_time_window\n",
        "            )\n",
        "            SELECT \n",
        "                window,\n",
        "                city,\n",
        "                avg_delivery_time,\n",
        "                orders_in_window,\n",
        "                AVG(avg_delivery_time) OVER (PARTITION BY city ORDER BY window_start ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) as overall_avg_delivery_time,\n",
        "                avg_delivery_time - AVG(avg_delivery_time) OVER (PARTITION BY city ORDER BY window_start ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) as diff_from_overall_avg\n",
        "            FROM ordered_windows\n",
        "            ORDER BY window DESC \n",
        "            LIMIT 10\n",
        "        \"\"\").show(truncate=False)\n",
        "        \n",
        "        print(\"\\n--- Monthly Average Delivery Time by City (30-day windows) ---\")\n",
        "        spark.sql(\"\"\"\n",
        "            WITH ordered_windows AS (\n",
        "                SELECT \n",
        "                    window,\n",
        "                    city,\n",
        "                    avg_delivery_time,\n",
        "                    monthly_orders,\n",
        "                    window.start as window_start\n",
        "                FROM monthly_delivery_time\n",
        "            )\n",
        "            SELECT \n",
        "                window,\n",
        "                city,\n",
        "                avg_delivery_time,\n",
        "                monthly_orders,\n",
        "                AVG(avg_delivery_time) OVER (PARTITION BY city ORDER BY window_start ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) as overall_avg_delivery_time,\n",
        "                avg_delivery_time - AVG(avg_delivery_time) OVER (PARTITION BY city ORDER BY window_start ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) as diff_from_overall_avg\n",
        "            FROM ordered_windows\n",
        "            ORDER BY window DESC \n",
        "            LIMIT 10\n",
        "        \"\"\").show(truncate=False)\n",
        "        \n",
        "        sleep(10)\n",
        "        \n",
        "except KeyboardInterrupt:\n",
        "    city_query.stop()\n",
        "    delivery_query.stop()\n",
        "    monthly_delivery_time_query.stop()\n",
        "    spark.stop()\n",
        "except Exception as e:\n",
        "    city_query.stop()\n",
        "    delivery_query.stop()\n",
        "    monthly_delivery_time_query.stop()\n",
        "    spark.stop()\n",
        "    print(f\"Error: {e}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Stop the Spark context\n",
        "spark.stop()"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
